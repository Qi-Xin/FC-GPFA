{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# Test if GPU is available\n",
    "# Note that CUDA below 12.1 can have bugs\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import libraries\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "from numpy.fft import fft as fft\n",
    "from numpy.fft import ifft as ifft\n",
    "import pickle\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats\n",
    "from scipy.stats import wilcoxon, chi2\n",
    "import scipy.interpolate \n",
    "import scipy.signal\n",
    "from scipy import linalg\n",
    "from scipy.special import rel_entr\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.genmod.generalized_linear_model as smm\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my code\n",
    "import utility_functions as utils\n",
    "import GLM\n",
    "from DataLoader import Allen_dataset, Allen_dataloader_multi_session, Simple_dataloader_from_spikes\n",
    "from model_trainer import Trainer\n",
    "\n",
    "utils.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLM neuron dataset\n",
    "file_name = '/home/qix/user_data/EIF_simulation_dataset/synthetic_data_GLM.npz'\n",
    "data = np.load(file_name)\n",
    "spikes = data['spikes'][:,:,:]\n",
    "nneuron = spikes.shape[1]//2\n",
    "synthetic_GLM_dataloader = Simple_dataloader_from_spikes(\n",
    "    [spikes[:,:nneuron,:], spikes[:,nneuron:,:]],\n",
    "    npadding=50,\n",
    "    train_ratio=0.2,\n",
    "    val_ratio=0.05,\n",
    "    batch_size=64,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = synthetic_GLM_dataloader\n",
    "total_trials_train = 0\n",
    "for batch in dataset.test_loader:\n",
    "    total_trials_held_out += batch['spike_trains'].shape[2]\n",
    "\n",
    "spikes_held_out = np.zeros((300, 100, total_trials_held_out))\n",
    "trial_idx = 0\n",
    "for batch in dataset.test_loader:\n",
    "    spikes_held_out[:, :, trial_idx:trial_idx+batch['spike_trains'].shape[2]] = batch['spike_trains']\n",
    "    trial_idx += batch['spike_trains'].shape[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for all ablation experiments\n",
    "verbose = False\n",
    "dataset = synthetic_GLM_dataloader\n",
    "nrep = 100\n",
    "ckp_path = '/home/qix/user_data/VAETransformer_checkpoint_ci'\n",
    "\n",
    "params = {\n",
    "    # B-spline basis\n",
    "    'num_B_spline_basis': 10,\n",
    "    # Transformer VAE's settings\n",
    "    'downsample_factor': 10,\n",
    "    'transformer_num_layers': 2,\n",
    "    'transformer_d_model': 128,\n",
    "    'transformer_dim_feedforward': 512,\n",
    "    'transformer_vae_output_dim': 8,\n",
    "    'transformer_dropout': 0.0,\n",
    "    'transformer_nhead': 1,\n",
    "    'stimulus_nfactor': 2,\n",
    "    'stimulus_decoder_inter_dim_factor': 2,\n",
    "    'beta': 1.0,\n",
    "    'use_area_specific_decoder': True,\n",
    "    'use_area_specific_encoder': True,\n",
    "    'use_cls': False,\n",
    "    # Coupling's settings\n",
    "    'coupling_basis_peaks_max': 7,\n",
    "    'coupling_basis_num': 3,\n",
    "    'coupling_nsubspace': 1,\n",
    "    'use_self_coupling': True,\n",
    "    # Coupling strength latent's settings\n",
    "    'K_sigma2': 1.0,\n",
    "    'K_tau': 100,\n",
    "    'coupling_strength_nlatent': 1,\n",
    "    # Self-history's settings\n",
    "    'self_history_basis_peaks_max': 2,\n",
    "    'self_history_basis_num': 3,\n",
    "    'self_history_basis_nonlinear': 0.7,\n",
    "    # Penalty settings\n",
    "    'penalty_smoothing_spline': 1e3,\n",
    "    'penalty_coupling_subgroup': 1e-5,\n",
    "    'penalty_diff_loading': None,\n",
    "    'penalty_loading_similarity': None,\n",
    "    # Training settings\n",
    "    'batch_size': 64,\n",
    "    'sample_latent': False,\n",
    "    'lr': 1e-3,\n",
    "    'epoch_warm_up': 0,\n",
    "    'epoch_patience': 3,\n",
    "    'epoch_max': 200,\n",
    "    'tol': 1e-5,\n",
    "    'weight_decay': 0,\n",
    "    'lr_transformer': 1e-4,\n",
    "    'lr_sti': 1e-2,\n",
    "    'lr_cp': 1e-2,\n",
    "    'lr_self_history': 1e-2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:05<02:04,  1.54it/s]\n",
      "/home/qix/FC-GPFA/model_trainer.py:316: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(self.temp_best_model_path))\n",
      " 26%|██▌       | 51/200 [00:38<01:51,  1.34it/s]\n",
      "  2%|▎         | 5/200 [00:05<03:27,  1.06s/it]\n",
      "  2%|▏         | 3/200 [00:03<03:50,  1.17s/it]\n",
      "  2%|▏         | 3/200 [00:03<03:56,  1.20s/it]\n",
      "  2%|▏         | 3/200 [00:03<03:58,  1.21s/it]\n",
      "  2%|▏         | 3/200 [00:03<04:03,  1.23s/it]\n",
      "  2%|▏         | 3/200 [00:04<04:29,  1.37s/it]\n",
      "  2%|▏         | 3/200 [00:04<04:23,  1.34s/it]\n",
      "  2%|▏         | 3/200 [00:03<04:21,  1.33s/it]\n",
      "  0%|          | 1/200 [00:01<05:28,  1.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_cp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     55\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mmake_optimizer(frozen_params\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_latent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_converter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     56\u001b[0m                                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msti_readout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msti_decoder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msti_inhomo\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_stimulus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_coupling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_self_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_stimulus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_latents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     coupling_filters\u001b[38;5;241m.\u001b[39mappend(trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcoupling_filters_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     68\u001b[0m coupling_filters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(coupling_filters)\n",
      "File \u001b[0;32m~/FC-GPFA/model_trainer.py:236\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, verbose, record_results, include_stimulus, include_coupling, include_self_history, fix_stimulus, fix_latents)\u001b[0m\n\u001b[1;32m    234\u001b[0m train_loss_wo_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    235\u001b[0m total_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader\u001b[38;5;241m.\u001b[39mtrain_loader, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_batch(batch)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/allen/lib/python3.9/site-packages/tqdm/std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/FC-GPFA/DataLoader.py:36\u001b[0m, in \u001b[0;36mBatchIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_batch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_batch_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/FC-GPFA/DataLoader.py:115\u001b[0m, in \u001b[0;36mSimple_dataloader_from_spikes.get_batch\u001b[0;34m(self, current_batch_idx, split)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a batch of data for the given split and batch index\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batches\u001b[39m\u001b[38;5;124m'\u001b[39m)[current_batch_idx]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspike_trains\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspikes_full\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnneuron_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnneuron_list,\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_indices\n\u001b[1;32m    119\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Full model\n",
    "coupling_filters = []\n",
    "\n",
    "for irep in range(nrep):\n",
    "    trials_held_out_shuffled = np.random.choice(\n",
    "        np.arange(total_trials_held_out), \n",
    "        size=total_trials_held_out,\n",
    "        replace=True,\n",
    "    )\n",
    "    spikes_held_out_shuffled = spikes_held_out[:,:,trials_held_out_shuffled]\n",
    "    synthetic_GLM_dataloader = Simple_dataloader_from_spikes(\n",
    "        [spikes_held_out_shuffled[:,:nneuron,:], spikes_held_out_shuffled[:,nneuron:,:]],\n",
    "        npadding=50,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.1,\n",
    "        batch_size=64,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(dataset, ckp_path, params)\n",
    "\n",
    "    # First step: train the model with a trial-invariant stimulus effect\n",
    "    trainer.train(\n",
    "        include_stimulus=True,\n",
    "        include_coupling=False,\n",
    "        include_self_history=False,\n",
    "        fix_stimulus=True,\n",
    "        fix_latents=True,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    # Second step: train the model with a trial-varying stimulus effect\n",
    "    # trainer.make_optimizer(frozen_params=['sti_readout'])\n",
    "    trainer.make_optimizer(frozen_params=['sti_inhomo', ]) # We are fixing the trial-invariant stimulus effect\n",
    "    trainer.train(\n",
    "        include_stimulus=True,\n",
    "        include_coupling=False,\n",
    "        include_self_history=False,\n",
    "        fix_stimulus=False,\n",
    "        fix_latents=True,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "    # trainer.make_optimizer(frozen_params=[])\n",
    "    trainer.train(\n",
    "        include_stimulus=True,\n",
    "        include_coupling=True,\n",
    "        include_self_history=False,\n",
    "        fix_stimulus=False,\n",
    "        fix_latents=True,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    coupling_filters.append(trainer.model.coupling_filters_dict[\"0\"][0][1].detach().cpu().numpy())\n",
    "\n",
    "coupling_filters = np.stack(coupling_filters)\n",
    "\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ci_full_model.npy', coupling_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qix/anaconda3/envs/allen/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:05<02:02,  1.57it/s]\n",
      "/home/qix/FC-GPFA/model_trainer.py:316: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(self.temp_best_model_path))\n",
      " 28%|██▊       | 57/200 [00:46<01:56,  1.23it/s]\n",
      "  3%|▎         | 6/200 [00:07<04:13,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# model w/o encoder\n",
    "coupling_filters = []\n",
    "\n",
    "trainer = Trainer(dataset, ckp_path, params)\n",
    "\n",
    "# First step: train the model with a trial-invariant stimulus effect\n",
    "trainer.train(\n",
    "    include_stimulus=True,\n",
    "    include_coupling=False,\n",
    "    include_self_history=False,\n",
    "    fix_stimulus=True,\n",
    "    fix_latents=True,\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "# trainer.make_optimizer(frozen_params=[])\n",
    "trainer.train(\n",
    "    include_stimulus=True,\n",
    "    include_coupling=True,\n",
    "    include_self_history=False,\n",
    "    fix_stimulus=True,\n",
    "    fix_latents=True,\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "for irep in range(nrep):\n",
    "    trials_held_out_shuffled = np.random.choice(\n",
    "        np.arange(total_trials_held_out), \n",
    "        size=total_trials_held_out,\n",
    "        replace=True,\n",
    "    )\n",
    "    spikes_held_out_shuffled = spikes_held_out[:,:,trials_held_out_shuffled]\n",
    "    synthetic_GLM_dataloader = Simple_dataloader_from_spikes(\n",
    "        [spikes_held_out_shuffled[:,:nneuron,:], spikes_held_out_shuffled[:,nneuron:,:]],\n",
    "        npadding=50,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.2,\n",
    "        batch_size=64,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter',\n",
    "                                          'sti_readout', 'sti_decoder', 'sti_inhomo'])\n",
    "    trainer.train(\n",
    "        include_stimulus=True,\n",
    "        include_coupling=True,\n",
    "        include_self_history=False,\n",
    "        fix_stimulus=True,\n",
    "        fix_latents=True,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    coupling_filters.append()\n",
    "\n",
    "coupling_filters = np.stack(coupling_filters)\n",
    "\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ci_wo_encoder.npy', coupling_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\tFull Model\tW/o Encoder\tW/o Coupling\tW/o post spike\tRNN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Data 1\t0.20082 (0.00028)\t0.20203 (0.00007)\t0.20298 (0.00024)\t0.20704 (0.00039)\t0.20173 (0.00043)\n",
      "Data 2\t0.25232 (0.00007)\t0.25334 (0.00008)\t0.25737 (0.00073)\t0.26147 (0.00023)\t0.25259 (0.00004)\n"
     ]
    }
   ],
   "source": [
    "# Load results from different models\n",
    "ci_full = np.load('/home/qix/user_data/EIF_simulation_dataset/results_ablation_full_model.npy')\n",
    "ci_wo_encoder = np.load('/home/qix/user_data/EIF_simulation_dataset/results_ci_wo_encoder.npy') \n",
    "\n",
    "\n",
    "# Calculate mean and sem for each model and dataset\n",
    "def mean_sem(data):\n",
    "    return f\"{data.mean():.5f} ({data.std()/np.sqrt(len(data)):.5f})\"\n",
    "\n",
    "# Create table rows\n",
    "table_rows = []\n",
    "for i in range(len(datasets)):\n",
    "    row = [\n",
    "        mean_sem(results_full[i,:]),\n",
    "        mean_sem(results_wo_encoder[i,:]), \n",
    "        mean_sem(results_wo_coupling[i,:]),\n",
    "        mean_sem(results_wo_post_spike[i,:]),\n",
    "        mean_sem(results_rnn[i,:])\n",
    "    ]\n",
    "    table_rows.append(row)\n",
    "\n",
    "# Print table\n",
    "print(\"Dataset\\tFull Model\\tW/o Encoder\\tW/o Coupling\\tW/o post spike\\tRNN\")\n",
    "print(\"-\"*100)\n",
    "for i, row in enumerate(table_rows):\n",
    "    print(f\"Data {i+1}\\t{row[0]}\\t{row[1]}\\t{row[2]}\\t{row[3]}\\t{row[4]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
