{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# Test if GPU is available\n",
    "# Note that CUDA below 12.1 can have bugs\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import libraries\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "from numpy.fft import fft as fft\n",
    "from numpy.fft import ifft as ifft\n",
    "import pickle\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats\n",
    "from scipy.stats import wilcoxon, chi2\n",
    "import scipy.interpolate \n",
    "import scipy.signal\n",
    "from scipy import linalg\n",
    "from scipy.special import rel_entr\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.genmod.generalized_linear_model as smm\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qix/anaconda3/envs/allen/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import my code\n",
    "import utility_functions as utils\n",
    "import GLM\n",
    "from DataLoader import Allen_dataset, Allen_dataloader_multi_session, Simple_dataloader_from_spikes\n",
    "from model_trainer import Trainer\n",
    "\n",
    "utils.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLM neuron dataset\n",
    "file_name = '/home/qix/user_data/EIF_simulation_dataset/synthetic_data_GLM.npz'\n",
    "data = np.load(file_name)\n",
    "spikes = data['spikes'][:,:,:500]\n",
    "nneuron = spikes.shape[1]//2\n",
    "synthetic_GLM_dataloader = Simple_dataloader_from_spikes(\n",
    "    [spikes[:,:nneuron,:], spikes[:,nneuron:,:]],\n",
    "    npadding=50,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.1,\n",
    "    batch_size=64,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Load EIF neuron dataset\n",
    "file_name = f'/home/qix/user_data/EIF_simulation_dataset/synthetic_data_EIF_connTrue.npz'\n",
    "data = np.load(file_name, allow_pickle=True)\n",
    "spikes = data['spikes'][:,:,:500]\n",
    "nneuron = spikes.shape[1]//2\n",
    "synthetic_EIF_dataloader = Simple_dataloader_from_spikes(\n",
    "    [spikes[:,:nneuron,:], spikes[:,nneuron:,:]],\n",
    "    npadding=50,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.1,\n",
    "    batch_size=64,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Load real dataset\n",
    "if sys.platform == 'linux':\n",
    "    data_path = '/home/qix/user_data/allen_spike_trains/single_sessions.joblib'\n",
    "else:\n",
    "    data_path = 'D:/ecephys_cache_dir/single_sessions.joblib'\n",
    "real_dataloader = joblib.load(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for all ablation experiments\n",
    "verbose = False\n",
    "datasets = [synthetic_GLM_dataloader, synthetic_EIF_dataloader]\n",
    "# datasets = [synthetic_GLM_dataloader, synthetic_EIF_dataloader, real_dataloader]\n",
    "nrep = 3\n",
    "ckp_path = '/home/qix/user_data/VAETransformer_checkpoint_ablation'\n",
    "\n",
    "params_set = {}\n",
    "params_set[0] = {\n",
    "    # B-spline basis\n",
    "    'num_B_spline_basis': 10,\n",
    "    # Transformer VAE's settings\n",
    "    'downsample_factor': 10,\n",
    "    'transformer_num_layers': 2,\n",
    "    'transformer_d_model': 128,\n",
    "    'transformer_dim_feedforward': 512,\n",
    "    'transformer_vae_output_dim': 8,\n",
    "    'transformer_dropout': 0.0,\n",
    "    'transformer_nhead': 1,\n",
    "    'stimulus_nfactor': 2,\n",
    "    'stimulus_decoder_inter_dim_factor': 2,\n",
    "    'beta': 1.0,\n",
    "    'use_area_specific_decoder': True,\n",
    "    'use_area_specific_encoder': True,\n",
    "    'use_cls': False,\n",
    "    # Coupling's settings\n",
    "    'coupling_basis_peaks_max': 7,\n",
    "    'coupling_basis_num': 3,\n",
    "    'coupling_nsubspace': 1,\n",
    "    'use_self_coupling': True,\n",
    "    # Coupling strength latent's settings\n",
    "    'K_sigma2': 1.0,\n",
    "    'K_tau': 100,\n",
    "    'coupling_strength_nlatent': 1,\n",
    "    # Self-history's settings\n",
    "    'self_history_basis_peaks_max': 2,\n",
    "    'self_history_basis_num': 3,\n",
    "    'self_history_basis_nonlinear': 0.7,\n",
    "    # Penalty settings\n",
    "    'penalty_smoothing_spline': 1e3,\n",
    "    'penalty_coupling_subgroup': 1e-5,\n",
    "    'penalty_diff_loading': None,\n",
    "    'penalty_loading_similarity': None,\n",
    "    # Training settings\n",
    "    'batch_size': 64,\n",
    "    'sample_latent': False,\n",
    "    'lr': 1e-3,\n",
    "    'epoch_warm_up': 0,\n",
    "    'epoch_patience': 3,\n",
    "    'epoch_max': 200,\n",
    "    'tol': 1e-5,\n",
    "    'weight_decay': 0,\n",
    "    'lr_transformer': 1e-4,\n",
    "    'lr_sti': 1e-2,\n",
    "    'lr_cp': 1e-2,\n",
    "    'lr_self_history': 1e-2,\n",
    "}\n",
    "\n",
    "params_set[1] = {\n",
    "    # B-spline basis\n",
    "    'num_B_spline_basis': 20,\n",
    "    # Transformer VAE's settings\n",
    "    'downsample_factor': 10,\n",
    "    'transformer_num_layers': 2,\n",
    "    'transformer_d_model': 128,\n",
    "    'transformer_dim_feedforward': 512,\n",
    "    'transformer_vae_output_dim': 16,\n",
    "    'transformer_dropout': 0.0,\n",
    "    'transformer_nhead': 1,\n",
    "    'stimulus_nfactor': 1,\n",
    "    'stimulus_decoder_inter_dim_factor': 2,\n",
    "    'beta': 1.0,\n",
    "    'use_area_specific_decoder': True,\n",
    "    'use_area_specific_encoder': True,\n",
    "    'use_cls': False,\n",
    "    # Coupling's settings\n",
    "    'coupling_basis_peaks_max': 5,\n",
    "    'coupling_basis_num': 3,\n",
    "    'coupling_nsubspace': 1,\n",
    "    'use_self_coupling': True,\n",
    "    # Coupling strength latent's settings\n",
    "    'K_sigma2': 1.0,\n",
    "    'K_tau': 100,\n",
    "    'coupling_strength_nlatent': 1,\n",
    "    # Self-history's settings\n",
    "    'self_history_basis_peaks_max': 1.5,\n",
    "    'self_history_basis_num': 3,\n",
    "    'self_history_basis_nonlinear': 1,\n",
    "    # Penalty settings\n",
    "    'penalty_smoothing_spline': 1e3,\n",
    "    'penalty_coupling_subgroup': 1e-5,\n",
    "    'penalty_diff_loading': None,\n",
    "    'penalty_loading_similarity': None,\n",
    "    # Training settings\n",
    "    'batch_size': 64,\n",
    "    'sample_latent': False,\n",
    "    'lr': 1e-3,\n",
    "    'epoch_warm_up': 0,\n",
    "    'epoch_patience': 3,\n",
    "    'epoch_max': 200,\n",
    "    'tol': 1e-5,\n",
    "    'weight_decay': 0,\n",
    "    'lr_transformer': 1e-4,\n",
    "    'lr_sti': 1e-2,\n",
    "    'lr_cp': 1e-2,\n",
    "    'lr_self_history': 1e-2,\n",
    "}\n",
    "\n",
    "params_set[2] = {\n",
    "    # B-spline basis\n",
    "    'num_B_spline_basis': 20,\n",
    "    # Transformer VAE's settings\n",
    "    'downsample_factor': 10,\n",
    "    'transformer_num_layers': 2,\n",
    "    'transformer_d_model': 128,\n",
    "    'transformer_dim_feedforward': 512,\n",
    "    'transformer_vae_output_dim': 16,\n",
    "    'transformer_dropout': 0.0,\n",
    "    'transformer_nhead': 1,\n",
    "    'stimulus_nfactor': 1,\n",
    "    'stimulus_decoder_inter_dim_factor': 2,\n",
    "    'beta': 1.0,\n",
    "    'use_area_specific_decoder': True,\n",
    "    'use_area_specific_encoder': True,\n",
    "    'use_cls': False,\n",
    "    # Coupling's settings\n",
    "    'coupling_basis_peaks_max': 5,\n",
    "    'coupling_basis_num': 3,\n",
    "    'coupling_nsubspace': 1,\n",
    "    'use_self_coupling': True,\n",
    "    # Coupling strength latent's settings\n",
    "    'K_sigma2': 1.0,\n",
    "    'K_tau': 100,\n",
    "    'coupling_strength_nlatent': 1,\n",
    "    # Self-history's settings\n",
    "    'self_history_basis_peaks_max': 1.5,\n",
    "    'self_history_basis_num': 3,\n",
    "    'self_history_basis_nonlinear': 1,\n",
    "    # Penalty settings\n",
    "    'penalty_smoothing_spline': 1e2,\n",
    "    'penalty_coupling_subgroup': 1e-5,\n",
    "    'penalty_diff_loading': None,\n",
    "    'penalty_loading_similarity': None,\n",
    "    # Training settings\n",
    "    'batch_size': 64,\n",
    "    'sample_latent': False,\n",
    "    'lr': 1e-3,\n",
    "    'epoch_warm_up': 0,\n",
    "    'epoch_patience': 3,\n",
    "    'epoch_max': 200,\n",
    "    'tol': 1e-5,\n",
    "    'weight_decay': 0,\n",
    "    'lr_transformer': 1e-4,\n",
    "    'lr_sti': 1e-2,\n",
    "    'lr_cp': 1e-2,\n",
    "    'lr_self_history': 1e-2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:00<00:15, 13.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 53/200 [00:04<00:12, 12.21it/s]\n",
      "/home/qix/FC-GPFA/model_trainer.py:316: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(self.temp_best_model_path))\n",
      " 53%|█████▎    | 106/200 [00:11<00:10,  8.87it/s]\n",
      " 10%|█         | 20/200 [00:02<00:23,  7.77it/s]\n",
      " 27%|██▋       | 54/200 [00:07<00:20,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [00:03<00:11, 12.91it/s]\n",
      " 22%|██▏       | 43/200 [00:04<00:15, 10.09it/s]\n",
      " 18%|█▊        | 36/200 [00:03<00:17,  9.27it/s]\n",
      " 28%|██▊       | 55/200 [00:06<00:15,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 52/200 [00:03<00:10, 14.04it/s]\n",
      " 34%|███▎      | 67/200 [00:07<00:14,  9.29it/s]\n",
      "  9%|▉         | 18/200 [00:01<00:18,  9.84it/s]\n",
      " 27%|██▋       | 54/200 [00:05<00:16,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 63/200 [00:04<00:10, 13.20it/s]\n",
      " 22%|██▏       | 43/200 [00:05<00:20,  7.80it/s]\n",
      "  8%|▊         | 16/200 [00:01<00:19,  9.31it/s]\n",
      " 28%|██▊       | 56/200 [00:06<00:15,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 72/200 [00:05<00:09, 13.00it/s]\n",
      " 20%|██        | 41/200 [00:04<00:15,  9.95it/s]\n",
      " 13%|█▎        | 26/200 [00:02<00:20,  8.67it/s]\n",
      " 28%|██▊       | 56/200 [00:06<00:16,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [00:04<00:09, 13.81it/s]\n",
      " 22%|██▎       | 45/200 [00:04<00:15,  9.93it/s]\n",
      " 10%|█         | 21/200 [00:02<00:22,  7.82it/s]\n",
      " 28%|██▊       | 56/200 [00:07<00:18,  7.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Full model\n",
    "results_ablation = np.zeros((len(datasets), nrep))\n",
    "\n",
    "for idata, data_to_use in enumerate(datasets):\n",
    "    for irep in range(nrep):\n",
    "\n",
    "        trainer = Trainer(data_to_use, ckp_path, params_set[idata])\n",
    "        \n",
    "        # First step: train the model with a trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=True,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        # Second step: train the model with a trial-varying stimulus effect\n",
    "        # trainer.make_optimizer(frozen_params=['sti_readout'])\n",
    "        trainer.make_optimizer(frozen_params=['sti_inhomo', ]) # We are fixing the trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter',\n",
    "            'sti_readout', 'sti_decoder', 'sti_inhomo', 'cp_latents_readout', 'cp_time_varying_coef_offset', \n",
    "            'cp_beta_coupling', 'cp_weight_sending', 'cp_weight_receiving'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        test_loss = trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=True,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        results_ablation[idata, irep] = test_loss\n",
    "\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ablation_full_model.npy', results_ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 54/200 [00:04<00:12, 11.64it/s]\n",
      " 40%|████      | 80/200 [00:09<00:13,  8.76it/s]\n",
      " 29%|██▉       | 58/200 [00:07<00:17,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 54/200 [00:04<00:11, 13.01it/s]\n",
      " 50%|█████     | 100/200 [00:10<00:10,  9.41it/s]\n",
      " 28%|██▊       | 57/200 [00:06<00:16,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/200 [00:04<00:13, 11.04it/s]\n",
      " 31%|███       | 62/200 [00:07<00:15,  8.63it/s]\n",
      " 28%|██▊       | 57/200 [00:06<00:16,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [00:05<00:10, 12.14it/s]\n",
      " 35%|███▌      | 70/200 [00:07<00:13,  9.89it/s]\n",
      " 28%|██▊       | 56/200 [00:06<00:16,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 63/200 [00:04<00:10, 13.39it/s]\n",
      " 20%|██        | 41/200 [00:04<00:19,  8.32it/s]\n",
      " 28%|██▊       | 56/200 [00:06<00:17,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 75/200 [00:05<00:09, 12.85it/s]\n",
      " 27%|██▋       | 54/200 [00:06<00:16,  8.71it/s]\n",
      " 28%|██▊       | 57/200 [00:06<00:17,  8.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Without Transformer encoder model\n",
    "results_ablation = np.zeros((len(datasets), nrep))\n",
    "\n",
    "for idata, data_to_use in enumerate(datasets):\n",
    "    for irep in range(nrep):\n",
    "\n",
    "        trainer = Trainer(data_to_use, ckp_path, params_set[idata])\n",
    "        \n",
    "        # First step: train the model with a trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=True,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter',\n",
    "            'sti_readout', 'sti_decoder', 'sti_inhomo', 'cp_latents_readout', 'cp_time_varying_coef_offset', \n",
    "            'cp_beta_coupling', 'cp_weight_sending', 'cp_weight_receiving'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        test_loss = trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=True,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        results_ablation[idata, irep] = test_loss\n",
    "\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ablation_wo_encoder.npy', results_ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 48/200 [00:03<00:11, 13.04it/s]\n",
      " 29%|██▉       | 58/200 [00:05<00:13, 10.36it/s]\n",
      " 32%|███▏      | 63/200 [00:06<00:14,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 55/200 [00:04<00:12, 11.81it/s]\n",
      " 37%|███▋      | 74/200 [00:08<00:15,  8.35it/s]\n",
      " 31%|███       | 62/200 [00:06<00:14,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 45/200 [00:03<00:12, 11.97it/s]\n",
      " 46%|████▌     | 91/200 [00:09<00:11,  9.14it/s]\n",
      " 30%|██▉       | 59/200 [00:06<00:14,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [00:05<00:10, 12.79it/s]\n",
      " 12%|█▏        | 24/200 [00:02<00:16, 10.41it/s]\n",
      " 30%|██▉       | 59/200 [00:05<00:13, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 64/200 [00:05<00:11, 12.31it/s]\n",
      " 22%|██▎       | 45/200 [00:05<00:18,  8.19it/s]\n",
      " 27%|██▋       | 54/200 [00:05<00:13, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 70/200 [00:05<00:10, 12.55it/s]\n",
      " 17%|█▋        | 34/200 [00:02<00:14, 11.48it/s]\n",
      " 29%|██▉       | 58/200 [00:05<00:14,  9.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Without coupling\n",
    "results_ablation = np.zeros((len(datasets), nrep))\n",
    "\n",
    "for idata, data_to_use in enumerate(datasets):\n",
    "    for irep in range(nrep):\n",
    "\n",
    "        trainer = Trainer(data_to_use, ckp_path, params_set[idata])\n",
    "        \n",
    "        # First step: train the model with a trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=True,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        # Second step: train the model with a trial-varying stimulus effect\n",
    "        # trainer.make_optimizer(frozen_params=['sti_readout'])\n",
    "        trainer.make_optimizer(frozen_params=['sti_inhomo', ]) # We are fixing the trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter',\n",
    "            'sti_readout', 'sti_decoder', 'sti_inhomo', 'cp_latents_readout', 'cp_time_varying_coef_offset', \n",
    "            'cp_beta_coupling', 'cp_weight_sending', 'cp_weight_receiving'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        test_loss = trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=True,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        results_ablation[idata, irep] = test_loss\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ablation_wo_coupling.npy', results_ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 54/200 [00:04<00:11, 12.31it/s]\n",
      " 34%|███▍      | 69/200 [00:07<00:13,  9.69it/s]\n",
      "  9%|▉         | 18/200 [00:02<00:23,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/200 [00:04<00:12, 12.24it/s]\n",
      " 45%|████▌     | 90/200 [00:10<00:12,  8.82it/s]\n",
      "  8%|▊         | 16/200 [00:02<00:24,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [00:04<00:12, 12.22it/s]\n",
      " 20%|██        | 41/200 [00:04<00:19,  8.29it/s]\n",
      "  8%|▊         | 17/200 [00:01<00:20,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 77/200 [00:05<00:09, 12.97it/s]\n",
      " 23%|██▎       | 46/200 [00:04<00:15, 10.14it/s]\n",
      "  8%|▊         | 17/200 [00:02<00:24,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [00:05<00:08, 14.12it/s]\n",
      " 19%|█▉        | 38/200 [00:03<00:15, 10.54it/s]\n",
      " 10%|█         | 20/200 [00:02<00:21,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [00:05<00:08, 13.60it/s]\n",
      "  6%|▋         | 13/200 [00:01<00:15, 11.75it/s]\n",
      " 10%|█         | 21/200 [00:02<00:23,  7.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Without neuron's post-spike effects\n",
    "results_ablation = np.zeros((len(datasets), nrep))\n",
    "\n",
    "for idata, data_to_use in enumerate(datasets):\n",
    "    for irep in range(nrep):\n",
    "\n",
    "        trainer = Trainer(data_to_use, ckp_path, params_set[idata])\n",
    "        \n",
    "        # First step: train the model with a trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=True,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        # Second step: train the model with a trial-varying stimulus effect\n",
    "        # trainer.make_optimizer(frozen_params=['sti_readout'])\n",
    "        trainer.make_optimizer(frozen_params=['sti_inhomo', ]) # We are fixing the trial-invariant stimulus effect\n",
    "        test_loss = trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        results_ablation[idata, irep] = test_loss\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ablation_wo_post_spike.npy', results_ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 43/200 [00:02<00:10, 14.75it/s]\n",
      " 55%|█████▌    | 110/200 [00:10<00:08, 10.28it/s]\n",
      " 12%|█▎        | 25/200 [00:02<00:18,  9.41it/s]\n",
      " 28%|██▊       | 55/200 [00:05<00:14,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [00:03<00:10, 14.35it/s]\n",
      "  8%|▊         | 17/200 [00:01<00:16, 11.23it/s]\n",
      " 24%|██▎       | 47/200 [00:04<00:15, 10.06it/s]\n",
      " 30%|██▉       | 59/200 [00:05<00:13, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 53/200 [00:03<00:09, 14.72it/s]\n",
      " 34%|███▎      | 67/200 [00:05<00:11, 11.33it/s]\n",
      " 14%|█▎        | 27/200 [00:02<00:14, 11.98it/s]\n",
      " 29%|██▉       | 58/200 [00:06<00:15,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 69/200 [00:04<00:08, 15.11it/s]\n",
      " 28%|██▊       | 57/200 [00:05<00:13, 10.52it/s]\n",
      " 23%|██▎       | 46/200 [00:04<00:15, 10.19it/s]\n",
      " 28%|██▊       | 56/200 [00:04<00:12, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 67/200 [00:03<00:07, 17.06it/s]\n",
      " 30%|██▉       | 59/200 [00:06<00:14,  9.64it/s]\n",
      " 12%|█▏        | 23/200 [00:02<00:18,  9.72it/s]\n",
      " 28%|██▊       | 55/200 [00:06<00:16,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 65/200 [00:04<00:08, 15.67it/s]\n",
      " 27%|██▋       | 54/200 [00:04<00:13, 10.87it/s]\n",
      " 14%|█▎        | 27/200 [00:02<00:15, 10.92it/s]\n",
      " 28%|██▊       | 55/200 [00:05<00:15,  9.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Transformer -> RNN\n",
    "results_ablation = np.zeros((len(datasets), nrep))\n",
    "\n",
    "for idata, data_to_use in enumerate(datasets):\n",
    "    for irep in range(nrep):\n",
    "\n",
    "        trainer = Trainer(data_to_use, ckp_path, params_set[idata])\n",
    "        \n",
    "        # First step: train the model with a trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=True,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        # Second step: train the model with a trial-varying stimulus effect\n",
    "        # trainer.make_optimizer(frozen_params=['sti_readout'])\n",
    "        trainer.make_optimizer(frozen_params=['sti_inhomo', ]) # We are fixing the trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter',\n",
    "            'sti_readout', 'sti_decoder', 'sti_inhomo', 'cp_latents_readout', 'cp_time_varying_coef_offset', \n",
    "            'cp_beta_coupling', 'cp_weight_sending', 'cp_weight_receiving'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        test_loss = trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=True,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        results_ablation[idata, irep] = test_loss\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ablation_rnn.npy', results_ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qix/anaconda3/envs/allen/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:04<01:50,  1.73it/s]\n",
      "/home/qix/FC-GPFA/model_trainer.py:316: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(self.temp_best_model_path))\n",
      " 24%|██▎       | 47/200 [00:34<01:51,  1.38it/s]\n",
      "  4%|▎         | 7/200 [00:06<03:04,  1.05it/s]\n",
      "  6%|▌         | 12/200 [00:11<03:01,  1.04it/s]\n",
      "/home/qix/anaconda3/envs/allen/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:04<01:40,  1.90it/s]\n",
      " 18%|█▊        | 37/200 [00:27<02:00,  1.35it/s]\n",
      "  6%|▌         | 11/200 [00:10<02:57,  1.07it/s]\n",
      "  6%|▌         | 12/200 [00:12<03:09,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# Low-rank -> full-rank\n",
    "results_ablation = np.zeros((len(datasets), nrep))\n",
    "\n",
    "for idata, data_to_use in enumerate(datasets):\n",
    "    for irep in range(nrep):\n",
    "\n",
    "        trainer = Trainer(data_to_use, ckp_path, params_set[idata])\n",
    "        \n",
    "        # First step: train the model with a trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=True,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        # Second step: train the model with a trial-varying stimulus effect\n",
    "        # trainer.make_optimizer(frozen_params=['sti_readout'])\n",
    "        trainer.make_optimizer(frozen_params=['sti_inhomo', ]) # We are fixing the trial-invariant stimulus effect\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=False,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=False,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter'])\n",
    "        trainer.make_optimizer(frozen_params=['transformer_encoder', 'to_latent', 'token_converter',\n",
    "            'sti_readout', 'sti_decoder', 'sti_inhomo', 'cp_latents_readout', 'cp_time_varying_coef_offset', \n",
    "            'cp_beta_coupling', 'cp_weight_sending', 'cp_weight_receiving'])\n",
    "        # trainer.make_optimizer(frozen_params=[])\n",
    "        test_loss = trainer.train(\n",
    "            include_stimulus=True,\n",
    "            include_coupling=True,\n",
    "            include_self_history=True,\n",
    "            fix_stimulus=False,\n",
    "            fix_latents=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        results_ablation[idata, irep] = test_loss\n",
    "np.save('/home/qix/user_data/EIF_simulation_dataset/results_ablation_full_rank.npy', results_ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\tFull Model\tW/o Encoder\tW/o Coupling\tW/o post spike\tRNN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Data 1\t0.20082 (0.00028)\t0.20203 (0.00007)\t0.20298 (0.00024)\t0.20704 (0.00039)\t0.20173 (0.00043)\n",
      "Data 2\t0.25232 (0.00007)\t0.25334 (0.00008)\t0.25737 (0.00073)\t0.26147 (0.00023)\t0.25259 (0.00004)\n"
     ]
    }
   ],
   "source": [
    "# Load results from different models\n",
    "results_full = np.load('/home/qix/user_data/EIF_simulation_dataset/results_ablation_full_model.npy')\n",
    "results_wo_encoder = np.load('/home/qix/user_data/EIF_simulation_dataset/results_ablation_wo_encoder.npy') \n",
    "results_wo_coupling = np.load('/home/qix/user_data/EIF_simulation_dataset/results_ablation_wo_coupling.npy')\n",
    "results_wo_post_spike = np.load('/home/qix/user_data/EIF_simulation_dataset/results_ablation_wo_post_spike.npy')\n",
    "results_rnn = np.load('/home/qix/user_data/EIF_simulation_dataset/results_ablation_rnn.npy')\n",
    "\n",
    "# Calculate mean and sem for each model and dataset\n",
    "def mean_sem(data):\n",
    "    return f\"{data.mean():.5f} ({data.std()/np.sqrt(len(data)):.5f})\"\n",
    "\n",
    "# Create table rows\n",
    "table_rows = []\n",
    "for i in range(len(datasets)):\n",
    "    row = [\n",
    "        mean_sem(results_full[i,:]),\n",
    "        mean_sem(results_wo_encoder[i,:]), \n",
    "        mean_sem(results_wo_coupling[i,:]),\n",
    "        mean_sem(results_wo_post_spike[i,:]),\n",
    "        mean_sem(results_rnn[i,:])\n",
    "    ]\n",
    "    table_rows.append(row)\n",
    "\n",
    "# Print table\n",
    "print(\"Dataset\\tFull Model\\tW/o Encoder\\tW/o Coupling\\tW/o post spike\\tRNN\")\n",
    "print(\"-\"*100)\n",
    "for i, row in enumerate(table_rows):\n",
    "    print(f\"Data {i+1}\\t{row[0]}\\t{row[1]}\\t{row[2]}\\t{row[3]}\\t{row[4]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
