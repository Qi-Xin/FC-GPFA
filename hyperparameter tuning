import numpy as np
import socket
import utility_functions as utils
import pickle
from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials
from model_trainer import Trainer

### Load data
session_id = 757216464
stimuli_name = ''
# stimuli_name = 'gabors'

hostname = socket.gethostname()
if hostname[:8] == "ghidorah":
    path_prefix = '/home'
elif hostname[:6] == "wright":
    path_prefix = '/home/export'
ckp_path = path_prefix+'/qix/user_data/FC-GPFA_checkpoint'
with open(path_prefix+'/qix/user_data/allen_spike_trains/'+str(session_id)+'.pkl', 'rb') as f:
    spikes = pickle.load(f)

# score below is actually the best test loss achieve when training the model, so the lower the better
best_score = [float('inf')]
def try_hp(params):
    trainer = Trainer(spikes, ckp_path, params)
    try:
        score = trainer.train(verbose=False)
        score = np.random.rand()
        if score < best_score[0]:
            best_score[0] = score
            print(f"New best score: {best_score[0]}")
            print(f"with params: {params}")
            trainer.save_model_and_hp()
        return {'loss': score, 'status': STATUS_OK}
    except Exception as e:
        print(f"An error occurred during training: {str(e)}")
        return {'loss': float('inf'), 'status': STATUS_FAIL}

# Define the search space
param_dist = {
    'num_merge': hp.choice('num_merge', [2, 5, 10, 20]),
    'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),
    'num_B_spline_basis': hp.choice('num_B_spline_basis', [10, 25, 50]),
    'nl_dim': hp.choice('nl_dim', [1, 3, 5, 10]),
    'num_layers': hp.choice('num_layers', [2, 4, 8]),
    'dim_feedforward': hp.choice('dim_feedforward', [32, 64, 128]),
    'nfactor': hp.choice('nfactor', [1, 3, 6]),
    'nhead': hp.choice('nhead', [1, 2, 4]),
    'learning_rate': hp.choice('learning_rate', [1e-4, 1e-3, 1e-2, 1e-1]),
    'learning_rate_decoder': hp.choice('learning_rate_decoder', [1e-4, 1e-3, 1e-2, 1e-1]),
    'dropout': hp.choice('dropout', [0, 0.2, 0.4, 0.6, 0.8]),
    'warm_up_epoch': hp.choice('warm_up_epoch', [3, 5, 10]),
    'max_epoch': hp.choice('max_epoch', [100]),
    'patience_epoch': hp.choice('patience_epoch', [3])
}


# Create a trials object to store details of each iteration
trials = Trials()

# Perform the optimization using Bayesian optimization
best = fmin(
    fn=try_hp,
    space=param_dist,
    algo=tpe.suggest,  # Use Bayesian optimization
    max_evals=10,  # Set the maximum number of evaluations
    trials=trials
)

# Extract the best parameters found during the optimization
print(f"Best Parameters: {best.items()}")